{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "Ziyi Jiang | zjia631 | 634926886"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "The working is shown in the report.\n",
    "\n",
    "#### Frequent Itemsets:\n",
    "There are 35 frequent itemsets.\n",
    "\n",
    "{A}, {B}, {C}, {D}, {E}, {F}, <br/>\n",
    "{A,B}, {A,C}, {A,D}, {A,F}, {B,C}, {B,D}, {B,E}, {B,F}, {C,D}, {C,E}, {C,F}, {D,F}, <br/>\n",
    "{A,B,C}, {A,B,D}, {A,B,F}, {A,C,D}, {A,C,F}, {A,D,F}, {B,C,D}, {B,C,E}, {B,C,F}, {B,D,F}, {C,D,F}, <br/>\n",
    "{A,B,C,D}, {A,B,C,F}, {A,B,D,F}, {A,C,D,F}, {B,C,D,F}, <br/>\n",
    "{A,B,C,D,F}\n",
    "\n",
    "#### Rules:\n",
    "There are 97 association rule.\n",
    "\n",
    "{A} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {B} → {A} &nbsp;&nbsp;&nbsp;&nbsp; {B} → {C} &nbsp;&nbsp;&nbsp;&nbsp; {C} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {D} → {A} &nbsp;&nbsp;&nbsp;&nbsp; {D} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {D} → {C} &nbsp;&nbsp;&nbsp;&nbsp;<br/>\n",
    "{E} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {E} → {C} &nbsp;&nbsp;&nbsp;&nbsp; {F} → {A} &nbsp;&nbsp;&nbsp;&nbsp; {F} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {F} → {C} &nbsp;&nbsp;&nbsp;&nbsp; {F} → {D}\n",
    "\n",
    "{A, C} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {A, C} → {D} &nbsp;&nbsp;&nbsp;&nbsp; {A, D} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {A, D} → {C} &nbsp;&nbsp;&nbsp;&nbsp; {A, F} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {A, F} → {C} &nbsp;&nbsp;&nbsp;&nbsp; {A, F} → {D} &nbsp;&nbsp;&nbsp;&nbsp; {B, D} → {A} &nbsp;&nbsp;&nbsp;&nbsp; {B, D} → {C} &nbsp;&nbsp;&nbsp;&nbsp; {B, E} → {C}<br/>\n",
    "{B, F} → {A} &nbsp;&nbsp;&nbsp;&nbsp; {B, F} → {C} &nbsp;&nbsp;&nbsp;&nbsp; {B, F} → {D} &nbsp;&nbsp;&nbsp;&nbsp; {D, C} → {A} &nbsp;&nbsp;&nbsp;&nbsp; {D, C} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {C, E} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {F, C} → {A} &nbsp;&nbsp;&nbsp;&nbsp; {F, C} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {F, C} → {D} &nbsp;&nbsp;&nbsp;&nbsp; {D} → {A, B}<br/>\n",
    "{D} → {A, C} &nbsp;&nbsp;&nbsp;&nbsp; {D} → {B, C} &nbsp;&nbsp;&nbsp;&nbsp; {D, F} → {A} &nbsp;&nbsp;&nbsp;&nbsp; {D, F} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {D, F} → {C} &nbsp;&nbsp;&nbsp;&nbsp; {E} → {B, C} &nbsp;&nbsp;&nbsp;&nbsp; {F} → {A, B} &nbsp;&nbsp;&nbsp;&nbsp; {F} → {A, C} &nbsp;&nbsp;&nbsp;&nbsp; {F} → {A, D} &nbsp;&nbsp;&nbsp;&nbsp; {F} → {B, C}<br/>\n",
    "{F} → {B, D} &nbsp;&nbsp;&nbsp;&nbsp; {F} → {D, C}    \n",
    "\n",
    "{A, B, C} → {D} &nbsp;&nbsp;&nbsp;&nbsp; {A, B, D} → {C} &nbsp;&nbsp;&nbsp;&nbsp; {A, B, F} → {C} &nbsp;&nbsp;&nbsp;&nbsp; {A, B, F} → {D} &nbsp;&nbsp;&nbsp;&nbsp; {A, C} → {B, D} &nbsp;&nbsp;&nbsp;&nbsp; {A, D, C} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {F, A, C} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {F, A, C} → {D}<br/>\n",
    "{A, D} → {B, C} &nbsp;&nbsp;&nbsp;&nbsp; {A, D, F} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {A, D, F} → {C} &nbsp;&nbsp;&nbsp;&nbsp; {A, F} → {B, C} &nbsp;&nbsp;&nbsp;&nbsp; {A, F} → {B, D} &nbsp;&nbsp;&nbsp;&nbsp; {A, F} → {D, C} &nbsp;&nbsp;&nbsp;&nbsp; {B, D, C} → {A} &nbsp;&nbsp;&nbsp;&nbsp; {F, B, C} → {A}<br/>\n",
    "{F, B, C} → {D} &nbsp;&nbsp;&nbsp;&nbsp; {B, D} → {A, C} &nbsp;&nbsp;&nbsp;&nbsp; {B, D, F} → {A} &nbsp;&nbsp;&nbsp;&nbsp; {B, D, F} → {C} &nbsp;&nbsp;&nbsp;&nbsp; {B, F} → {A, C} &nbsp;&nbsp;&nbsp;&nbsp; {B, F} → {A, D} &nbsp;&nbsp;&nbsp;&nbsp; {B, F} → {D, C} &nbsp;&nbsp;&nbsp;&nbsp; {D, C} → {A, B}<br/>\n",
    "{F, D, C} → {A} &nbsp;&nbsp;&nbsp;&nbsp; {F, D, C} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {F, C} → {A, B} &nbsp;&nbsp;&nbsp;&nbsp; {F, C} → {A, D} &nbsp;&nbsp;&nbsp;&nbsp; {F, C} → {B, D} &nbsp;&nbsp;&nbsp;&nbsp; {D} → {A, B, C} &nbsp;&nbsp;&nbsp;&nbsp; {D, F} → {A, B} &nbsp;&nbsp;&nbsp;&nbsp; {D, F} → {A, C}<br/>\n",
    "{D, F} → {B, C} &nbsp;&nbsp;&nbsp;&nbsp; {F} → {A, B, C} &nbsp;&nbsp;&nbsp;&nbsp; {F} → {A, B, D} &nbsp;&nbsp;&nbsp;&nbsp; {F} → {A, D, C} &nbsp;&nbsp;&nbsp;&nbsp; {F} → {B, D, C}\n",
    "\n",
    "{F, A, B, C} → {D} &nbsp;&nbsp;&nbsp;&nbsp; {A, B, D, F} → {C} &nbsp;&nbsp;&nbsp;&nbsp; {A, B, F} → {D, C} &nbsp;&nbsp;&nbsp;&nbsp; {F, A, D, C} → {B} &nbsp;&nbsp;&nbsp;&nbsp; {F, A, C} → {B, D} &nbsp;&nbsp;&nbsp;&nbsp; {A, D, F} → {B, C} &nbsp;&nbsp;&nbsp;&nbsp; {A, F} → {B, D, C}<br/>\n",
    "{F, B, D, C} → {A} &nbsp;&nbsp;&nbsp;&nbsp; {F, B, C} → {A, D} &nbsp;&nbsp;&nbsp;&nbsp; {B, D, F} → {A, C} &nbsp;&nbsp;&nbsp;&nbsp; {B, F} → {A, D, C} &nbsp;&nbsp;&nbsp;&nbsp; {F, D, C} → {A, B} &nbsp;&nbsp;&nbsp;&nbsp; {F, C} → {A, B, D} &nbsp;&nbsp;&nbsp;&nbsp; {D, F} → {A, B, C}<br/>\n",
    "{F} → {A, B, D, C}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Apriori:\n",
    "\n",
    "    def __init__(self, minsup, minconf, minlift, dataset):\n",
    "        self.minsup = minsup\n",
    "        self.minconf = minconf\n",
    "        self.minlift = minlift\n",
    "        self.total_tran_num = len(dataset)\n",
    "        self.dataset = dataset\n",
    "        self.freq_itemsets = {}\n",
    "        self.assoc_rules = []\n",
    "\n",
    "    def candi_itemsets_gen(self, freq_itemsets):\n",
    "        candi_itemsets = set()\n",
    "        freq_itemsets_num = len(freq_itemsets)\n",
    "        if freq_itemsets_num > 1:\n",
    "            max_length = len(freq_itemsets[0]) + 1\n",
    "            all_index_combinations = itertools.combinations(range(freq_itemsets_num), 2)\n",
    "            for index1, index2 in all_index_combinations:\n",
    "                candi_itemset = freq_itemsets[index1] | freq_itemsets[index2]\n",
    "                if len(candi_itemset) == max_length:\n",
    "                    candi_itemsets.add(candi_itemset)\n",
    "        return candi_itemsets\n",
    "\n",
    "    def freq_itemsets_gen(self, candi_itemsets):\n",
    "        one_turn_freq_itemsets = []\n",
    "        support_count_dict = {}\n",
    "        if len(candi_itemsets) == 0:\n",
    "            return one_turn_freq_itemsets\n",
    "        for transaction in self.dataset:\n",
    "            for itemset in candi_itemsets:\n",
    "                if itemset.issubset(frozenset(transaction)):\n",
    "                    support_count_dict[itemset] = support_count_dict.get(itemset, 0) + 1\n",
    "        for itemset in support_count_dict:\n",
    "            support = support_count_dict[itemset] / self.total_tran_num\n",
    "            if support >= self.minsup:\n",
    "                self.freq_itemsets[itemset] = support\n",
    "                one_turn_freq_itemsets.append(itemset)\n",
    "        return one_turn_freq_itemsets\n",
    "\n",
    "    def find_freq_itemsets(self):\n",
    "        # get frequent itemset with length 1\n",
    "        whole_data = np.concatenate(self.dataset)\n",
    "        len1_candi_itemsets_counts = np.unique(whole_data, return_counts=True)\n",
    "        unique_items = list(len1_candi_itemsets_counts[0])\n",
    "        len1_candi_itemsets_counts_dict = dict(zip(unique_items, len1_candi_itemsets_counts[1]))\n",
    "        for itemset in len1_candi_itemsets_counts_dict:\n",
    "            support = len1_candi_itemsets_counts_dict[itemset] / self.total_tran_num\n",
    "            if support >= self.minsup:\n",
    "                self.freq_itemsets[frozenset([itemset])] = support\n",
    "        # get frequent itemset with length greater than 1\n",
    "        one_turn_freq_itemsets = list(self.freq_itemsets.keys())\n",
    "        while len(one_turn_freq_itemsets) > 0:\n",
    "            candi_itemsets = self.candi_itemsets_gen(one_turn_freq_itemsets)\n",
    "            one_turn_freq_itemsets = self.freq_itemsets_gen(candi_itemsets)\n",
    "\n",
    "    def find_assoc_rules(self):\n",
    "        for itemset in self.freq_itemsets:\n",
    "            if len(itemset) > 1:\n",
    "                for length in range(1, len(itemset)):\n",
    "                    antecedents = itertools.combinations(itemset, length)\n",
    "                    for antecedent in antecedents:\n",
    "                        support = self.freq_itemsets[itemset]\n",
    "                        antecedent = frozenset(antecedent)\n",
    "                        consequent = itemset - antecedent\n",
    "                        confidence = support / self.freq_itemsets[antecedent]\n",
    "                        lift = confidence / self.freq_itemsets[consequent]\n",
    "                        if confidence >= self.minconf and lift >= self.minlift:\n",
    "                            self.assoc_rules.append([antecedent, consequent, lift, confidence, support])\n",
    "\n",
    "    def get_freq_itemsets(self):\n",
    "        self.find_freq_itemsets()\n",
    "        decoded_freq_itemsets = []\n",
    "        for codeset in self.freq_itemsets:\n",
    "            freq_itemset = []\n",
    "            for code in codeset:\n",
    "                freq_itemset.append(code)\n",
    "            decoded_freq_itemsets.append([freq_itemset, self.freq_itemsets[codeset]])\n",
    "        return decoded_freq_itemsets\n",
    "\n",
    "    def get_assoc_rules(self):\n",
    "        self.find_assoc_rules()\n",
    "        decoded_freq_assoc_rules = []\n",
    "        for rule in self.assoc_rules:\n",
    "            antecedent = []\n",
    "            consequent = []\n",
    "            for code in rule[0]:\n",
    "                antecedent.append(code)\n",
    "            for code in rule[1]:\n",
    "                consequent.append(code)\n",
    "            lift = rule[2]\n",
    "            conf = rule[3]\n",
    "            support = rule[4]\n",
    "            decoded_freq_assoc_rules.append([antecedent, consequent, lift, conf, support])\n",
    "        decoded_freq_assoc_rules = sorted(decoded_freq_assoc_rules, key=lambda x: (len(x[0] + x[1]), x[2], x[3], x[4]), reverse=True)\n",
    "        return decoded_freq_assoc_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Frequent Itemsets:\n",
      "['A'] support: 0.8\n",
      "['B'] support: 1.0\n",
      "['C'] support: 0.8\n",
      "['D'] support: 0.6\n",
      "['E'] support: 0.2\n",
      "['F'] support: 0.2\n",
      "['D', 'C'] support: 0.6\n",
      "['C', 'B'] support: 0.8\n",
      "['F', 'C'] support: 0.2\n",
      "['A', 'C'] support: 0.6\n",
      "['A', 'F'] support: 0.2\n",
      "['F', 'B'] support: 0.2\n",
      "['A', 'B'] support: 0.8\n",
      "['D', 'B'] support: 0.6\n",
      "['A', 'D'] support: 0.6\n",
      "['D', 'F'] support: 0.2\n",
      "['E', 'C'] support: 0.2\n",
      "['E', 'B'] support: 0.2\n",
      "['D', 'C', 'A'] support: 0.6\n",
      "['F', 'C', 'A'] support: 0.2\n",
      "['A', 'C', 'B'] support: 0.6\n",
      "['F', 'C', 'B'] support: 0.2\n",
      "['A', 'D', 'F'] support: 0.2\n",
      "['A', 'B', 'F'] support: 0.2\n",
      "['F', 'D', 'B'] support: 0.2\n",
      "['A', 'D', 'B'] support: 0.6\n",
      "['D', 'C', 'B'] support: 0.6\n",
      "['D', 'C', 'F'] support: 0.2\n",
      "['E', 'C', 'B'] support: 0.2\n",
      "['D', 'A', 'C', 'F'] support: 0.2\n",
      "['D', 'B', 'F', 'C'] support: 0.2\n",
      "['D', 'B', 'A', 'C'] support: 0.6\n",
      "['D', 'B', 'A', 'F'] support: 0.2\n",
      "['B', 'F', 'C', 'A'] support: 0.2\n",
      "['D', 'F', 'C', 'B', 'A'] support: 0.2\n",
      "====================================================================================================\n",
      "Association Rules:\n",
      "['F'] -> ['D', 'C', 'B', 'A'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['D', 'F'] -> ['A', 'C', 'B'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'C'] -> ['D', 'B', 'A'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'B'] -> ['D', 'C', 'A'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'A'] -> ['D', 'C', 'B'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['D', 'B', 'F'] -> ['A', 'C'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'C', 'B'] -> ['D', 'A'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'C', 'A'] -> ['D', 'B'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'B', 'A'] -> ['D', 'C'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'C', 'B', 'A'] -> ['D'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['D', 'C', 'F'] -> ['A', 'B'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['D', 'A', 'F'] -> ['C', 'B'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['D', 'C', 'B', 'F'] -> ['A'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['D', 'A', 'B', 'F'] -> ['C'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['D', 'A', 'C', 'F'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.2\n",
      "['D'] -> ['A', 'C', 'B'] lift: 1.6666666666666667 confidence: 1.0 support: 0.6\n",
      "['D', 'B'] -> ['A', 'C'] lift: 1.6666666666666667 confidence: 1.0 support: 0.6\n",
      "['A', 'C'] -> ['D', 'B'] lift: 1.6666666666666667 confidence: 1.0 support: 0.6\n",
      "['A', 'C', 'B'] -> ['D'] lift: 1.6666666666666667 confidence: 1.0 support: 0.6\n",
      "['F'] -> ['D', 'C', 'A'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['D', 'F'] -> ['A', 'C'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['A', 'F'] -> ['D', 'C'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'C'] -> ['D', 'A'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['A', 'C', 'F'] -> ['D'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F'] -> ['D', 'C', 'B'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'B'] -> ['D', 'C'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'C'] -> ['D', 'B'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'C', 'B'] -> ['D'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F'] -> ['D', 'B', 'A'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'B'] -> ['D', 'A'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['A', 'F'] -> ['D', 'B'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['A', 'B', 'F'] -> ['D'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F'] -> ['A', 'C', 'B'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'B'] -> ['A', 'C'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['D', 'A'] -> ['C', 'B'] lift: 1.25 confidence: 1.0 support: 0.6\n",
      "['D', 'C'] -> ['A', 'B'] lift: 1.25 confidence: 1.0 support: 0.6\n",
      "['D', 'B', 'A'] -> ['C'] lift: 1.25 confidence: 1.0 support: 0.6\n",
      "['D', 'C', 'B'] -> ['A'] lift: 1.25 confidence: 1.0 support: 0.6\n",
      "['D', 'A', 'F'] -> ['C'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['D', 'C', 'F'] -> ['A'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['D', 'F'] -> ['C', 'B'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['D', 'B', 'F'] -> ['C'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['D', 'F'] -> ['A', 'B'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['D', 'B', 'F'] -> ['A'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['F', 'C'] -> ['A', 'B'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['F', 'A'] -> ['C', 'B'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['F', 'C', 'B'] -> ['A'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['F', 'B', 'A'] -> ['C'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['D', 'C', 'A'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.6\n",
      "['D', 'C', 'F'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.2\n",
      "['D', 'A', 'F'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.2\n",
      "['F', 'C', 'A'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.2\n",
      "['D'] -> ['A', 'C'] lift: 1.6666666666666667 confidence: 1.0 support: 0.6\n",
      "['A', 'C'] -> ['D'] lift: 1.6666666666666667 confidence: 1.0 support: 0.6\n",
      "['F'] -> ['A', 'C'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F'] -> ['A', 'D'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['A', 'F'] -> ['D'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F'] -> ['D', 'B'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'B'] -> ['D'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F'] -> ['D', 'C'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['F', 'C'] -> ['D'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['D', 'C'] -> ['A'] lift: 1.25 confidence: 1.0 support: 0.6\n",
      "['D', 'A'] -> ['C'] lift: 1.25 confidence: 1.0 support: 0.6\n",
      "['D'] -> ['A', 'B'] lift: 1.25 confidence: 1.0 support: 0.6\n",
      "['D', 'B'] -> ['A'] lift: 1.25 confidence: 1.0 support: 0.6\n",
      "['D'] -> ['C', 'B'] lift: 1.25 confidence: 1.0 support: 0.6\n",
      "['D', 'B'] -> ['C'] lift: 1.25 confidence: 1.0 support: 0.6\n",
      "['F', 'C'] -> ['A'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['F', 'A'] -> ['C'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['F'] -> ['C', 'B'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['F', 'B'] -> ['C'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['D', 'F'] -> ['A'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['F'] -> ['A', 'B'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['F', 'B'] -> ['A'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['D', 'F'] -> ['C'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['E'] -> ['C', 'B'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['E', 'B'] -> ['C'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['A', 'C'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.6\n",
      "['A', 'D'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.6\n",
      "['D', 'C'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.6\n",
      "['F', 'C'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.2\n",
      "['A', 'F'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.2\n",
      "['F', 'D'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.2\n",
      "['E', 'C'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.2\n",
      "['F'] -> ['D'] lift: 1.6666666666666667 confidence: 1.0 support: 0.2\n",
      "['D'] -> ['C'] lift: 1.25 confidence: 1.0 support: 0.6\n",
      "['D'] -> ['A'] lift: 1.25 confidence: 1.0 support: 0.6\n",
      "['F'] -> ['C'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['F'] -> ['A'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['E'] -> ['C'] lift: 1.25 confidence: 1.0 support: 0.2\n",
      "['C'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.8\n",
      "['A'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.8\n",
      "['D'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.6\n",
      "['F'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.2\n",
      "['E'] -> ['B'] lift: 1.0 confidence: 1.0 support: 0.2\n",
      "['B'] -> ['C'] lift: 1.0 confidence: 0.8 support: 0.8\n",
      "['B'] -> ['A'] lift: 1.0 confidence: 0.8 support: 0.8\n",
      "====================================================================================================\n",
      "Number of frequent itemsets: 35\n",
      "Number of association rules: 97\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "task1_data = [['A', 'B', 'C', 'D', 'F'], ['A', 'B', 'C', 'D'], ['A', 'B', 'C', 'D'], ['A', 'B'], ['B', 'C', 'E']]\n",
    "\n",
    "task3_apriori = Apriori(0.15, 0.8, 0, task1_data)\n",
    "task3_itemsets = task3_apriori.get_freq_itemsets()\n",
    "task3_rules = task3_apriori.get_assoc_rules()\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"Frequent Itemsets:\")\n",
    "for itemset in task3_itemsets:\n",
    "    print(itemset[0], \"support:\", itemset[1])\n",
    "print(\"=\" * 100)\n",
    "print(\"Association Rules:\")\n",
    "for rule in task3_rules:\n",
    "    print(rule[0], \"->\", rule[1], \"lift:\", rule[2], \"confidence:\", rule[3], \"support:\", rule[4])\n",
    "print(\"=\" * 100)\n",
    "print(\"Number of frequent itemsets:\", len(task3_itemsets))\n",
    "print(\"Number of association rules:\", len(task3_rules))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "def read_csv_file(file_name):\n",
    "    items = []\n",
    "    with open(file_name) as file:\n",
    "        reader = csv.reader(file, delimiter=\",\")\n",
    "        for row in reader:\n",
    "            items.append(row)\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarket_dataset = read_csv_file(\"supermarket.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task4_apriori_1 = Apriori(0.1, 0.8, 2, supermarket_dataset)\n",
    "task4_itemsets_1 = task4_apriori_1.get_freq_itemsets()\n",
    "task4_rules_1 = task4_apriori_1.get_assoc_rules()\n",
    "\n",
    "task4_apriori_2 = Apriori(0.35, 0.8, 1, supermarket_dataset)\n",
    "task4_itemsets_2 = task4_apriori_2.get_freq_itemsets()\n",
    "task4_rules_2 = task4_apriori_2.get_assoc_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task4_itemsets_dataframe_1 = pd.DataFrame(columns=[\"itemset\", \"support\"])\n",
    "task4_rules_dataframe_1 = pd.DataFrame(columns=[\"antecedent\", \"consequent\", \"lift\", \"confidence\", \"support\"])\n",
    "if len(task4_itemsets_1) != 0:\n",
    "    task4_itemsets_dataframe_1 = pd.DataFrame(np.array(task4_itemsets_1, dtype=object), columns=[\"itemset\", \"support\"])\n",
    "if len(task4_rules_1) != 0:\n",
    "    task4_rules_dataframe_1 = pd.DataFrame(np.array(task4_rules_1, dtype=object), columns=[\"antecedent\", \"consequent\", \"lift\", \"confidence\", \"support\"])\n",
    "\n",
    "task4_itemsets_dataframe_2 = pd.DataFrame(columns=[\"itemset\", \"support\"])\n",
    "task4_rules_dataframe_2 = pd.DataFrame(columns=[\"antecedent\", \"consequent\", \"lift\", \"confidence\", \"support\"])\n",
    "if len(task4_itemsets_2) != 0:\n",
    "    task4_itemsets_dataframe_2 = pd.DataFrame(np.array(task4_itemsets_2, dtype=object), columns=[\"itemset\", \"support\"])\n",
    "if len(task4_rules_2) != 0:\n",
    "    task4_rules_dataframe_2 = pd.DataFrame(np.array(task4_rules_2, dtype=object), columns=[\"antecedent\", \"consequent\", \"lift\", \"confidence\", \"support\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "['baking needs', 'frozen foods', 'sauces-gravy-pkle', 'tissues-paper prd', 'biscuits'] -> ['total = high'] lift: 2.2564970438269003 confidence: 0.8188153310104529 support: 0.10157769613140263\n",
      "['frozen foods', 'sauces-gravy-pkle', 'tissues-paper prd', 'bread and cake', 'biscuits'] -> ['total = high'] lift: 2.2551687512408174 confidence: 0.8183333333333332 support: 0.1061162740436568\n",
      "['baking needs', 'frozen foods', 'sauces-gravy-pkle', 'tissues-paper prd', 'bread and cake'] -> ['total = high'] lift: 2.24909412284578 confidence: 0.8161290322580645 support: 0.10935811540955262\n",
      "['baking needs', 'sauces-gravy-pkle', 'tissues-paper prd', 'bread and cake', 'biscuits'] -> ['total = high'] lift: 2.2370668815471393 confidence: 0.8117647058823529 support: 0.1043872919818457\n",
      "['vegetables', 'sauces-gravy-pkle', 'tissues-paper prd', 'bread and cake', 'biscuits'] -> ['total = high'] lift: 2.2169356708896197 confidence: 0.8044596912521442 support: 0.10136157337367625\n",
      "['vegetables', 'frozen foods', 'sauces-gravy-pkle', 'tissues-paper prd', 'bread and cake'] -> ['total = high'] lift: 2.2136810552729473 confidence: 0.8032786885245902 support: 0.1059001512859304\n",
      "Mean lift: 2.2380739209372003 Mean confidence: 0.8121301303768229 Mean support: 0.10478351703767741\n",
      "====================================================================================================\n",
      "['fruit', 'milk-cream'] -> ['bread and cake'] lift: 1.148135574239598 confidence: 0.8263002944062807 support: 0.3639507240112384\n",
      "['vegetables', 'milk-cream'] -> ['bread and cake'] lift: 1.137665962258555 confidence: 0.8187654320987655 support: 0.3583315323103523\n",
      "['vegetables', 'fruit'] -> ['bread and cake'] lift: 1.1275829975875284 confidence: 0.8115088355233347 support: 0.38707585908796194\n",
      "['margarine'] -> ['bread and cake'] lift: 1.111955968205968 confidence: 0.8002622377622377 support: 0.3957207693970175\n",
      "Mean lift: 1.1313351255729123 Mean confidence: 0.8142091999476546 Mean support: 0.37626972120164254\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 100)\n",
    "\n",
    "ave_lift = 0\n",
    "ave_conf = 0\n",
    "ave_sup = 0\n",
    "for rule in task4_rules_1:\n",
    "    print(rule[0], \"->\", rule[1], \"lift:\", rule[2], \"confidence:\", rule[3], \"support:\", rule[4])\n",
    "    ave_lift += rule[2]\n",
    "    ave_conf += rule[3]\n",
    "    ave_sup += rule[4]\n",
    "print(\"Mean lift:\", ave_lift / len(task4_rules_1), \"Mean confidence:\", ave_conf / len(task4_rules_1), \"Mean support:\", ave_sup / len(task4_rules_1))\n",
    "\n",
    "print(\"=\" * 100)\n",
    "\n",
    "ave_lift = 0\n",
    "ave_conf = 0\n",
    "ave_sup = 0\n",
    "for rule in task4_rules_2:\n",
    "    print(rule[0], \"->\", rule[1], \"lift:\", rule[2], \"confidence:\", rule[3], \"support:\", rule[4])\n",
    "    ave_lift += rule[2]\n",
    "    ave_conf += rule[3]\n",
    "    ave_sup += rule[4]\n",
    "print(\"Mean lift:\", ave_lift / len(task4_rules_2), \"Mean confidence:\", ave_conf / len(task4_rules_2), \"Mean support:\", ave_sup / len(task4_rules_2))\n",
    "\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AprioriWithMinRelativeSup:\n",
    "\n",
    "    def __init__(self, minsup, minconf, minlift, minRelativeSup, dataset):\n",
    "        self.minsup = minsup\n",
    "        self.minconf = minconf\n",
    "        self.minlift = minlift\n",
    "        self.minRelativeSup = minRelativeSup\n",
    "        self.total_tran_num = len(dataset)\n",
    "        self.dataset = dataset\n",
    "        self.freq_itemsets = {}\n",
    "        self.relative_freq_itemsets = {}\n",
    "        self.assoc_rules = []\n",
    "\n",
    "    def candi_itemsets_gen(self, freq_itemsets):\n",
    "        candi_itemsets = set()\n",
    "        freq_itemsets_num = len(freq_itemsets)\n",
    "        if freq_itemsets_num > 1:\n",
    "            max_length = len(freq_itemsets[0]) + 1\n",
    "            all_index_combinations = itertools.combinations(range(freq_itemsets_num), 2)\n",
    "            for index1, index2 in all_index_combinations:\n",
    "                candi_itemset = freq_itemsets[index1] | freq_itemsets[index2]\n",
    "                if len(candi_itemset) == max_length:\n",
    "                    candi_itemsets.add(candi_itemset)\n",
    "        return candi_itemsets\n",
    "\n",
    "    def freq_itemsets_gen(self, candi_itemsets, pre_maxsubset):\n",
    "        one_turn_freq_itemsets = []\n",
    "        support_count_dict = {}\n",
    "        current_maxsubset = 0\n",
    "        check_relative = True\n",
    "        if len(candi_itemsets) == 0:\n",
    "            return one_turn_freq_itemsets, current_maxsubset\n",
    "        if len(list(candi_itemsets)[0]) <= 2:\n",
    "            check_relative = False\n",
    "        for transaction in self.dataset:\n",
    "            for itemset in candi_itemsets:\n",
    "                if itemset.issubset(frozenset(transaction)):\n",
    "                    support_count_dict[itemset] = support_count_dict.get(itemset, 0) + 1\n",
    "        for itemset in support_count_dict:\n",
    "            support = support_count_dict[itemset] / self.total_tran_num\n",
    "            if support > current_maxsubset:\n",
    "                current_maxsubset = support\n",
    "            if check_relative:\n",
    "                if support >= self.minsup:\n",
    "                    self.freq_itemsets[itemset] = support\n",
    "                    if support / pre_maxsubset >= self.minRelativeSup:\n",
    "                        self.relative_freq_itemsets[itemset] = support\n",
    "                    one_turn_freq_itemsets.append(itemset)\n",
    "            else:\n",
    "                if support >= self.minsup:\n",
    "                    self.freq_itemsets[itemset] = support\n",
    "                    self.relative_freq_itemsets[itemset] = support\n",
    "                    one_turn_freq_itemsets.append(itemset)\n",
    "\n",
    "        return one_turn_freq_itemsets, current_maxsubset\n",
    "\n",
    "    def find_freq_itemsets(self):\n",
    "        # get frequent itemset with length 1\n",
    "        whole_data = np.concatenate(self.dataset)\n",
    "        len1_candi_itemsets_counts = np.unique(whole_data, return_counts=True)\n",
    "        unique_items = list(len1_candi_itemsets_counts[0])\n",
    "        len1_candi_itemsets_counts_dict = dict(zip(unique_items, len1_candi_itemsets_counts[1]))\n",
    "        for itemset in len1_candi_itemsets_counts_dict:\n",
    "            support = len1_candi_itemsets_counts_dict[itemset] / self.total_tran_num\n",
    "            if support >= self.minsup:\n",
    "                self.freq_itemsets[frozenset([itemset])] = support\n",
    "        # get frequent itemset with length greater than 1\n",
    "        one_turn_freq_itemsets = list(self.freq_itemsets.keys())\n",
    "        maxsubset = None\n",
    "        while len(one_turn_freq_itemsets) > 0:\n",
    "            candi_itemsets = self.candi_itemsets_gen(one_turn_freq_itemsets)\n",
    "            one_turn_freq_itemsets, maxsubset = self.freq_itemsets_gen(candi_itemsets, maxsubset)\n",
    "\n",
    "    def find_assoc_rules(self):\n",
    "        for itemset in self.relative_freq_itemsets:\n",
    "            if len(itemset) > 1:\n",
    "                for length in range(1, len(itemset)):\n",
    "                    antecedents = itertools.combinations(itemset, length)\n",
    "                    for antecedent in antecedents:\n",
    "                        support = self.freq_itemsets[itemset]\n",
    "                        antecedent = frozenset(antecedent)\n",
    "                        consequent = itemset - antecedent\n",
    "                        confidence = support / self.freq_itemsets[antecedent]\n",
    "                        lift = confidence / self.freq_itemsets[consequent]\n",
    "                        if confidence >= self.minconf and lift >= self.minlift:\n",
    "                            self.assoc_rules.append([antecedent, consequent, lift, confidence, support])\n",
    "\n",
    "    def get_freq_itemsets(self):\n",
    "        self.find_freq_itemsets()\n",
    "        decoded_freq_itemsets = []\n",
    "        for codeset in self.relative_freq_itemsets:\n",
    "            freq_itemset = []\n",
    "            for code in codeset:\n",
    "                freq_itemset.append(code)\n",
    "            decoded_freq_itemsets.append([freq_itemset, self.relative_freq_itemsets[codeset]])\n",
    "        return decoded_freq_itemsets\n",
    "\n",
    "    def get_assoc_rules(self):\n",
    "        self.find_assoc_rules()\n",
    "        decoded_freq_assoc_rules = []\n",
    "        for rule in self.assoc_rules:\n",
    "            antecedent = []\n",
    "            consequent = []\n",
    "            for code in rule[0]:\n",
    "                antecedent.append(code)\n",
    "            for code in rule[1]:\n",
    "                consequent.append(code)\n",
    "            lift = rule[2]\n",
    "            conf = rule[3]\n",
    "            support = rule[4]\n",
    "            decoded_freq_assoc_rules.append([antecedent, consequent, lift, conf, support])\n",
    "        decoded_freq_assoc_rules = sorted(decoded_freq_assoc_rules, key=lambda x: (len(x[0] + x[1]), x[2], x[3], x[4]),\n",
    "                                          reverse=True)\n",
    "        return decoded_freq_assoc_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "supermarket_dataset = read_csv_file(\"supermarket.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "minsup = 0.15, minconf = 0.8, minlift = 1, minRelativeSup = 0.5\n",
      "frequent itemsets: 672\n",
      "association rules: 358\n",
      "====================================================================================================\n",
      "minsup = 0.15, minconf = 0.8, minlift = 1, minRelativeSup = 0\n",
      "frequent itemsets: 2022\n",
      "association rules: 955\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "task5_apriori_1 = AprioriWithMinRelativeSup(0.15, 0.8, 1, 0.5, supermarket_dataset)\n",
    "task5_apriori_2 = AprioriWithMinRelativeSup(0.15, 0.8, 1, 0, supermarket_dataset)\n",
    "\n",
    "task5_itemsets_1 = task5_apriori_1.get_freq_itemsets()\n",
    "task5_rules_1 = task5_apriori_1.get_assoc_rules()\n",
    "task5_itemsets_2 = task5_apriori_2.get_freq_itemsets()\n",
    "task5_rules_2 = task5_apriori_2.get_assoc_rules()\n",
    "\n",
    "task5_itemsets_dataframe_1 = pd.DataFrame(columns=[\"itemset\", \"support\"])\n",
    "task5_rules_dataframe_1 = pd.DataFrame(columns=[\"antecedent\", \"consequent\", \"lift\", \"confidence\", \"support\"])\n",
    "if len(task5_itemsets_1) != 0:\n",
    "    task5_itemsets_dataframe_1 = pd.DataFrame(np.array(task5_itemsets_1, dtype=object), columns=[\"itemset\", \"support\"])\n",
    "if len(task5_rules_1) != 0:\n",
    "    task5_rules_dataframe_1 = pd.DataFrame(np.array(task5_rules_1, dtype=object), columns=[\"antecedent\", \"consequent\", \"lift\", \"confidence\", \"support\"])\n",
    "task5_itemsets_dataframe_2 = pd.DataFrame(columns=[\"itemset\", \"support\"])\n",
    "task5_rules_dataframe_2 = pd.DataFrame(columns=[\"antecedent\", \"consequent\", \"lift\", \"confidence\", \"support\"])\n",
    "if len(task5_itemsets_2) != 0:\n",
    "    task5_itemsets_dataframe_2 = pd.DataFrame(np.array(task5_itemsets_2, dtype=object), columns=[\"itemset\", \"support\"])\n",
    "if len(task5_rules_2) != 0:\n",
    "    task5_rules_dataframe_2 = pd.DataFrame(np.array(task5_rules_2, dtype=object), columns=[\"antecedent\", \"consequent\", \"lift\", \"confidence\", \"support\"])\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"minsup = 0.15, minconf = 0.8, minlift = 1, minRelativeSup = 0.5\")\n",
    "print(\"frequent itemsets:\", len(task5_itemsets_1))\n",
    "print(\"association rules:\", len(task5_rules_1))\n",
    "print(\"=\" * 100)\n",
    "print(\"minsup = 0.15, minconf = 0.8, minlift = 1, minRelativeSup = 0\")\n",
    "print(\"frequent itemsets:\", len(task5_itemsets_2))\n",
    "print(\"association rules:\", len(task5_rules_2))\n",
    "print(\"=\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
